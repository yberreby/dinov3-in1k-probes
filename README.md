# DINOv3 ImageNet-1k Linear Classification Probes

Upon its release in August 2025, [DINOv3](https://github.com/facebookresearch/dinov3) marked a milestone in self-supervised representation learning for image processing.
The 7-billion-parameter flagship model was distilled into a family of smaller ViT and ConvNeXT checkpoints, whose sizes make them much more suitable for most CV tasks.

Sadly, only one ImageNet-1k (IN1k) linear classification probe was released: the one for the 7B model.

**Here, we release pretrained linear probes for the smaller DINOv3 ViT models.**
They can be used directly with Meta's official checkpoints.

As in the original DINOv3 paper, we used **512x512 inputs** (1024 input tokens),
and trained the probes on the IN1k training set with Inception-crop augmentation.

**All of our probes match or exceed the best IN1k-ReAL top-1 validation accuracy reported by the DINOv3 authors**, as seen in Table 14 of the original paper.

We note that the raw IN1k top-1 validation accuracy was not reported by the DINOv3 authors, only the [ReAL](https://github.com/google-research/reassessed-imagenet) top-1 accuracy.
Here, we report both.


## Released Probes

- **ViT-S/16** @ 512×512
  - Base: [`facebook/dinov3-vits16-pretrain-lvd1689m`](https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m)
  - Probe: [`yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe`](https://huggingface.co/yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe)

- **ViT-S+/16** @ 512×512
  - Base: [`facebook/dinov3-vits16plus-pretrain-lvd1689m`](https://huggingface.co/facebook/dinov3-vits16plus-pretrain-lvd1689m)
  - Probe: [`yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe`](https://huggingface.co/yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe)

- **ViT-B/16** @ 512×512
  - Base: [`facebook/dinov3-vitb16-pretrain-lvd1689m`](https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m)
  - Probe: [`yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe`](https://huggingface.co/yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe)

- **ViT-L/16** @ 512×512
  - Base: [`facebook/dinov3-vitl16-pretrain-lvd1689m`](https://huggingface.co/facebook/dinov3-vitl16-pretrain-lvd1689m)
  - Probe: [`yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe`](https://huggingface.co/yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe)

- **ViT-H+/16** @ 512×512
  - Base: [`facebook/dinov3-vith16plus-pretrain-lvd1689m`](https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m)
  - Probe: [`yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe`](https://huggingface.co/yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe)

See [the corresponding HuggingFace Collection](https://huggingface.co/collections/yberreby/dinov3-imagenet-1k-probes).

## Performance

| Probe | [IN-ReAL](https://github.com/google-research/reassessed-imagenet) val top-1 (official / ours) | IN1k val top-1 (ours) |
|-------|--------------------------------|-------------------|
| [ViT-S/16](https://huggingface.co/yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe) | 87.0% / **87.08%** | 81.40% |
| [ViT-S+/16](https://huggingface.co/yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe) | 88.0% / **88.08%** | 82.89% |
| [ViT-B/16](https://huggingface.co/yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe) | 89.3% / **89.54%** | 85.00% |
| [ViT-L/16](https://huggingface.co/yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe) | 90.2% / **90.42%** | 87.44% |
| [ViT-H+/16](https://huggingface.co/yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe) | 90.3% / **90.31%** | 87.65% |

The accuracy of the latest probes uploaded on the HF Hub can be queried using `uv run print_metrics.py`.

## Usage

We recommend using [`uv`](https://docs.astral.sh/uv/).

### Quick demo

Run the demo directly (no clone needed):

```bash
uv run https://raw.githubusercontent.com/yberreby/dinov3-in1k-probes/main/demo.py
```

### Using `from_pretrained`

```python
from dinov3_in1k_probes import DINOv3LinearClassificationHead

probe = DINOv3LinearClassificationHead.from_pretrained(
    "yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe"
)
# DINOv3LinearClassificationHead(in_features=768, out_features=1000, bias=True)
```

To get an interactive shell with the package:

```bash
uvx --with 'git+https://github.com/yberreby/dinov3-in1k-probes.git' ipython
```


## Development

To push to HuggingFace Hub, use the `push_to_hub.py` script.
It will auto-detect the model name, image size, and other metadata.

Example usage:

```bash
uv run push_to_hub.py --checkpoint dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe.pt
```
